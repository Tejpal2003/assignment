{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1fd77b-a7a4-4f8a-b031-843258b3dee2",
   "metadata": {},
   "source": [
    "# Answer 1\n",
    "\n",
    "Missing values refer to the absence of data in one or more columns or features of a dataset. These missing values may occur due to various reasons, such as data collection errors, data corruption, or data preprocessing issues.\n",
    "Some of the common algorithms that are not affected by missing values are:\n",
    "1. Decision Trees\n",
    "2. Random Forests\n",
    "3. Gradient Boosting Machines\n",
    "4. K-Nearest Neighbors\n",
    "5. Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063eabc-8f90-41a9-b1cc-69f98d72882d",
   "metadata": {},
   "source": [
    "# Answer  2\n",
    "\n",
    "1.  Deletion: Deletion involves removing the rows or columns that contain missing values from the dataset. This technique is only recommended when the amount of missing data is small relative to the size of the dataset, and the missing data is missing completely at random (MCAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57c3fbd-d21f-4e27-9b06-f49e41820921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B   C\n",
       "0  1.0  5.0   9\n",
       "3  4.0  8.0  12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, np.nan, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cf3f9-7c6c-40fc-b0fb-d16c40aef04c",
   "metadata": {},
   "source": [
    "2. Imputation: Imputation involves replacing the missing values with estimated values based on the available data. There are several methods of imputation, including mean imputation, median imputation, and mode imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55243c8-a785-4b4e-9689-27db638a08c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A    B   C\n",
       "0  1.000000  5.0   9\n",
       "1  2.000000  6.5  10\n",
       "2  2.333333  6.5  11\n",
       "3  4.000000  8.0  12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, np.nan, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b60b9-c3b1-4a89-98a4-8a87cfd0111e",
   "metadata": {},
   "source": [
    "3. K-Nearest Neighbor Imputation: K-Nearest Neighbor imputation involves replacing missing values with the average of the K-nearest neighbors in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab33955c-8aea-40b6-8f2f-5f9fa6ee5682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "0  1.0  5.0   9.0\n",
       "1  2.0  6.5  10.0\n",
       "2  3.0  6.5  11.0\n",
       "3  4.0  8.0  12.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4],\n",
    "                   'B': [5, np.nan, np.nan, 8],\n",
    "                   'C': [9, 10, 11, 12]})\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855e905-4768-4607-96e3-8a08060db8ed",
   "metadata": {},
   "source": [
    "# Answer 3\n",
    "\n",
    "Imbalanced data refers to a dataset where the number of instances in one class is significantly higher or lower than the number of instances in another class. In other words, the distribution of the target variable is not uniform.\n",
    "If imbalanced data is not handled, it can lead to several problems, including:\n",
    "1. Biased model performance: In the case of imbalanced data, a model may be biased towards the majority class because it has more data to learn from. This can result in poor performance on the minority class, which may be of greater importance in certain applications.\n",
    "2. False positives and false negatives: In imbalanced data, a model may predict the majority class with high accuracy, but perform poorly on the minority class. This can lead to a high number of false positives and false negatives.\n",
    "3. Overfitting: In the case of imbalanced data, a model may overfit to the majority class, resulting in poor generalization performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b6c29-02fd-4665-9ca6-2c471dadf40d",
   "metadata": {},
   "source": [
    "# Answer 4\n",
    "\n",
    "1. Up-sampling refers to the process of increasing the number of instances in the minority class by randomly duplicating them. This can be done using techniques such as random oversampling or SMOTE (Synthetic Minority Over-sampling Technique).For example, suppose we have a dataset with 100 instances, out of which 90 belong to class A and 10 belong to class B. Since the dataset is imbalanced, we can up-sample the minority class B by randomly duplicating its instances, resulting in a new dataset with 100 instances, out of which 90 belong to class A and 20 belong to class B.\n",
    "2. Down-sampling refers to the process of decreasing the number of instances in the majority class by randomly removing them. This can be done using techniques such as random undersampling or Tomek links. For example, suppose we have a dataset with 100 instances, out of which 90 belong to class A and 10 belong to class B. Since the dataset is imbalanced, we can down-sample the majority class A by randomly removing some of its instances, resulting in a new dataset with 50 instances, out of which 45 belong to class A and 5 belong to class B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f8609-bd4a-44c5-99f0-f79dbca6cb86",
   "metadata": {},
   "source": [
    "# Answer 5\n",
    "\n",
    "1. Data augmentation is a technique used to increase the size of a dataset by creating new, synthetic data from the original data. This is often done to address problems of overfitting, improve the generalization performance of machine learning models, or to balance an imbalanced dataset.\n",
    "1. SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique. SMOTE works by creating synthetic instances of the minority class by interpolating between existing instances of that class. Specifically, for each instance in the minority class, SMOTE selects k nearest neighbors (typically k=5) and creates a new instance by linearly interpolating between the selected instance and one of its k nearest neighbors. The interpolation factor is chosen randomly between 0 and 1, and the new instance is added to the dataset. SMOTE is an effective technique for handling imbalanced data, as it creates new synthetic instances that are similar to existing instances in the minority class, and can thus help the machine learning model generalize better to the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9076882-9630-4152-b496-79495f79e2a0",
   "metadata": {},
   "source": [
    "# Answer 6\n",
    "\n",
    "1. Outliers are data points in a dataset that significantly deviate from the rest of the data points. They can be caused by errors in data collection, measurement errors, or they may represent actual extreme values in the population.\n",
    "2. It is essential to handle outliers because they can have a significant impact on the performance of machine learning models. Outliers can affect the accuracy and generalization performance of a model, as they can bias the model towards the extreme values, leading to overfitting or underfitting. Outliers can also affect the results of statistical analysis, such as the mean, variance, and standard deviation, leading to inaccurate or misleading conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c039e40-d9d0-456f-aa01-c2b41bde0a37",
   "metadata": {},
   "source": [
    "# Answer 7\n",
    "\n",
    "\n",
    "There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "\n",
    "1. Deletion: One technique is to simply delete the rows or columns with missing data. This is only recommended if the missing data is a small percentage of the total dataset and if the missing data is completely at random. However, if the missing data is a large percentage of the dataset, this technique can result in a significant loss of information.\n",
    "2. Imputation: Another technique is to impute the missing values. This can be done using statistical methods such as mean imputation, median imputation, or mode imputation. Alternatively, more advanced techniques such as k-nearest neighbor imputation or multiple imputation can be used.\n",
    "3. Machine learning-based methods: Machine learning-based methods can also be used to handle missing data. For example, regression-based methods can be used to predict missing values based on other variables in the dataset.\n",
    "4. Domain knowledge: Domain knowledge can also be used to handle missing data. For example, if certain values are missing for a customer, but it is known that all customers with certain characteristics have the same value, then that value can be imputed for the missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b81510-7187-46e9-92f8-1ffe6747de27",
   "metadata": {},
   "source": [
    "# Answer 8\n",
    "\n",
    "There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data:\n",
    "1. Visualization: One strategy is to visualize the data using plots and graphs to see if there are any patterns or trends in the missing data. For example, a heatmap can be used to show which values are missing in the dataset.\n",
    "2. Summary statistics: Another strategy is to calculate summary statistics for the missing data and compare them to the summary statistics for the non-missing data. If there are significant differences between the two, this may suggest that the missing data is not missing at random.\n",
    "3. Imputation: Imputation can also be used to determine if the missing data is missing at random. If the imputed values are similar to the non-missing values, this may suggest that the missing data is missing at random. If the imputed values are significantly different, this may suggest that there is a pattern to the missing data.\n",
    "4. Statistical tests: Statistical tests can also be used to determine if the missing data is missing at random. For example, a chi-square test can be used to test whether the missing data is independent of other variables in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72dc1d7-6387-4b11-ae96-b982887bc9af",
   "metadata": {},
   "source": [
    "# Answer 9\n",
    "\n",
    "\n",
    "1. Confusion Matrix: A confusion matrix is a table that summarizes the performance of a classifier on a dataset. It displays the number of true positives, false positives, true negatives, and false negatives. A confusion matrix can help to evaluate the performance of the model, especially when dealing with imbalanced datasets.\n",
    "2. Precision, Recall, and F1-score: Precision, recall, and F1-score are metrics that are commonly used to evaluate the performance of machine learning models on imbalanced datasets. Precision is the fraction of true positive predictions among all positive predictions, while recall is the fraction of true positive predictions among all actual positive instances. F1-score is the harmonic mean of precision and recall. These metrics are useful when evaluating the performance of models on imbalanced datasets because they take into account both the false positive and false negative rates.\n",
    "3. ROC Curve and AUC: ROC (Receiver Operating Characteristic) curve is a plot of the true positive rate against the false positive rate. AUC (Area Under the ROC Curve) is a metric that measures the area under the ROC curve. ROC curve and AUC can be used to evaluate the performance of a model on an imbalanced dataset. A high AUC value suggests that the model is performing well on the dataset.\n",
    "4. Resampling techniques: Resampling techniques such as oversampling the minority class or undersampling the majority class can also be used to balance the dataset. Once the dataset is balanced, standard metrics such as accuracy, precision, recall, F1-score, and ROC can be used to evaluate the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88e85a-cab1-48bc-aa14-b33b8fc525ba",
   "metadata": {},
   "source": [
    "# Answer 10\n",
    "\n",
    "\n",
    "1. Random undersampling: This method involves randomly selecting a subset of observations from the majority class to match the size of the minority class. This can be done using techniques such as RandomUnderSampler from the imblearn library in Python.\n",
    "2. Tomek links: This method involves identifying pairs of observations that are nearest neighbors and belong to different classes. The observation from the majority class is then removed to balance the dataset. This can be done using techniques such as TomekLinks from the imblearn library in Python.\n",
    "3. Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class to match the size of the majority class. SMOTE can be used in combination with random undersampling to balance the dataset. This can be done using techniques such as SMOTETomek from the imblearn library in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa44cf-9c24-4bcd-b4f6-872270eca5a9",
   "metadata": {},
   "source": [
    "# Answer 11\n",
    "\n",
    "1. Random oversampling: This method involves randomly duplicating observations from the minority class to match the size of the majority class. This can be done using techniques such as RandomOverSampler from the imblearn library in Python.\n",
    "2. Synthetic minority oversampling technique (SMOTE): This method involves generating synthetic observations for the minority class to match the size of the majority class. SMOTE can be used in combination with random oversampling to balance the dataset. This can be done using techniques such as SMOTE from the imblearn library in Python.\n",
    "3. Adaptive synthetic sampling (ADASYN): This method is similar to SMOTE but focuses on generating more synthetic observations for the minority class samples that are harder to learn. This can be done using techniques such as ADASYN from the imblearn library in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc6a57-45f1-45a8-98f5-75b87236ab07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
